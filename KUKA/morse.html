<!DOCTYPE html>

<meta charset="utf-8">
<title>MORSE - The Modular OpenRobots Simulation Engine</title>
<link href="../media/js/prettify/prettify.css" type="text/css" rel="stylesheet" />
<script type="text/javascript" src="../media/js/prettify/prettify.js"></script>

<section>
    <h1>Introducing MORSE and an Approach for Low Cost Activity Recognition<br/></h1>
    <footer>
    
	<!--img style="float:left; margin:0px;width:auto;height:60px;" src="../media/logos/tum.svg"/-->
	<img style="float:left; margin:0px;width:auto;height:90px;" src="../media/logos/logo-tum-ias.png"/>
	
	
    Michael Karg<br /> October 2013</footer>
    
    <details>
        <ul>
            <li>Ende Promotion TUM</li>
            <li>paar Projekte vorstellen an denen ich während Diss gearbeitet habe...</li>
        </UL>
    </details>
</section>

<section>
    <h2>Presentation outline</h2>
    <ul>
      <li>The MORSE simulator
		<!--ul>
      		<li>Simulating with MORSE
      		<li>MORSE Principles
      		<li>Human-Robot Interaction
      		<li>Distributed simulations
      		<li>Still missing...
		</ul-->
	  <li>Low Cost Activity Recognition
		<!--ul>
			<li>A General Framework for expectations
			<li>Simulated Application Scenarios
			<li>Low Cost Activity Recognition
			<li>Real World Application 
		</ul-->
	  <li>Student Project: HRI Interface for B21
    </ul>
     <details>
		<ul>
            <li>... und zwar möchte Ich zuerst über den MORSE Sim reden, an dem ich in weiten Teilen mitentwickelt habe...</li>
            <li>... befor ich dann auf einen Ansatz zur Aktivitätenerkennung von Alltagsaktivitäten von Menschen Basierend auf Motion Tracking Daten eingehe.</li>
            <li>Zuletzt gehe ich noch kurz auf ein Projekt ein, das im Rahmen einer studentischen Arbeit entstanden ist und bei der es um eine Schnittstelle zur Steuerung
                eines Roboters mittels natürlicher Sprache geht.</li>
		</ul>
    </details>
</section>

<section>
    <h2>Presentation outline</h2>
    <ul>
        <li>The MORSE simulator
            <ul>
                <li>Simulating with MORSE
                    <li>MORSE Principles
                        <li>Human-Robot Interaction
                            <li>Distributed simulations
                                <li>Still missing...
                                    </ul>
            <li style="color: grey;">Low Cost Activity Recognition
                <!--ul>
                 <li>A General Framework for expectations
                 <li>Simulated Application Scenarios
                 <li>Low Cost Activity Recognition
                 <li>Real World Application
                 </ul-->
                <li style="color: grey;">Student Project: HRI Interface for B21
                    </ul>
    <details>
		<ul>
            <li>Structure of the talk as follows:</li>
            <li>First generally introduce the MORSE project and give a little example of it use, </li>
            <li>... then Pierrick will show you some more details and talk about HRI and distributed simulations.</li>
		</ul>
    </details>
</section>

<section>
	<figure>
        <video muted style="left:0px" src="../media/videos/morse-1.0.webm#t=7"></video>
    </figure>
    <details>
        <ul>
            <li>Beginnen möchte ich den Vortrag mit einem kurzen Video, das wir anläßlich der 1.0 Release von MORSE im Feb. 2013 geschnitten haben und eine gute Übersicht darüber gibt, was mit MORSE mittlerweile alles möglich ist..</li>
            
            <li>MORSE ist ein Open Source Projekt und basiert auf den Open-Source Projekten Blender, Bullet and Python</li>
            <li>MORSE kommt mit einigen Robotern (Quadcopter, mobile Manipulatoren, UBoot), verschienste Arten von Sensoren und Aktuatoren, die sich auf verschiedenste Arten kombinieren lassen.</li>
            <li>Beinhaltet Support für verschiedene Middlewares, von Sockets bis zu ROS, YARP, Pocolibs MOOS und mehr.</li>
            <li>Und beinhaltet einen Menschen, der sich steuern lässt wie in 3D Computerspielen für HRI</li>
            <li>Multnode simulation ermöglicht Simulation mehrerer Roboter auf mehrere Computer verteilt</li>
            <li>Besonders Einstieg in MORSE sollte einfach sein, deswegen viele Tutorials wie ...</li>
            <li>Einfach erweiterbar mit Python</li>
        </ul>
    </details>
</section>

<section>
    <h2>The MORSE project</h2>
    <ul>
      <li>100% open-source, permissive BSD-like license</li>
      <li>Release 1.0 in Feb. 2013</li>
      <li>~30K LOC, mostly Python</li>
    </ul>
    <details>
        <ul>
            <li>MORSE ist ein 100 prozentiges Open Source Projekt unter einer toleranten BSD ähnlichen Lizenz</li>
            <li>Februar diesen Jahres hatten wir unseren 1.0 Release mit ca. 30 K Zeilen Code, hauptsächlich in Python</li>
        </ul>
    </details>
</section>

<section>
    <h2>The MORSE community</h2>
    <ul>
      <li>Academic project, baked by academics
      <li>Initiated at LAAS-CNRS in 2009
      <li>As of Apr. 2013, contributions by 12 institutions worldwide
      <img style="width:100%" src="../media/logos/contributors.svg"/>
    </ul>
    
    <details>
		<ul>
            <li>MORSE ist als ein akademische Projekt entstanden und wurde 2009 am LAAS CNRS in Toulouse gestartet.</li>
            <li>... und beinhaltet mittlerweile Beiträge von 12 Institutionen weltweit ... </li>
		</ul>
    </details>
</section>

<section>
    <h2>The MORSE community</h2>
    <ul>
      <li>15 different individual contributors for the last releases
      <li>> 630 commits in 2013 only
      <li>> 130 users on <tt>morse-users</tt>
      <li> ... + the Blender and Bullet communities!
    </ul>
    <details>
        <ul>
            <li>MORSE hat eine aktive Community und beinhaltet Contributions von 15 verschiedenen Personen...</li>
            <li>... mit über 650 Commits alleine in 2013....</li>
            <li>... und 130 Usern auf der MORSE users mailingliste</li>
            <li>Außerdem haben BLender und Bullet selbst sehr aktive Communities.</li>
        </ul>
    </details>
</section>

<section>
    <h2>Blender Game Engine + Bullet</h2>
        <video style="position:absolute;top:190px; left:50px;height:220px" src="../media/videos/bge-yofrankie.webm"></video>
        <video style="position:absolute;top:190px; left:360px;height: 220px;" src="../media/videos/bullet.webm"></video>
		<div style="position:absolute;top:430px; left:390px;width: 320px; text-align: center;">Bullet Physics</div>
		<div style="position:absolute;top:430px; left:35px;width: 320px; text-align: center;">Blender Game Engine</div>
    <details>
        <ul>
            <li>Blender, auf dem MORSE basiert, ist eigentlich primär ein 3D Modellierungstool, bietet aber eine  3D game engine, die wir für MORSE verwenden.</li>
            <li>BGE: Geschrieben in C++, erweiterbar in Python + logic bricks .. </li>
            <li>... und ein nettes Feature dieser GE ist dass sie die Open Source Bullet Physics Engine standardmässig integriert hat.</li>
            <li>Bullet Physics bietet support für rigid bodies und soft bodies</li>
            <li>Switch from the modeller to the simulation == press 'P'</li>
            <li>limited support for dynamics (Vehicle class)</li>
        </ul>
    </details>
</section>


<section>
	<h2>Possibility to Exchange Physics Engine</h2>
	 <video style="position:absolute;top:170px; left:50px;width:320px" src="../media/videos/grasp_bullet.webm"></video>
	 <video style="position:absolute;top:170px; left:400px;width: 320px;" src="../media/videos/grasp_dvc3d.webm"></video>
		<div style="position:absolute;top:360px; left:400px;width: 320px; text-align: center;">DVC3D</div>
        <!--div style="position:absolute;top:400px; left:110px;width: 550px; text-align: center;font-size:0.4em;">(Video Courtesy of Binh Nguyen)</div-->
		<div style="position:absolute;top:360px; left:50px;width: 320px; text-align: center;">Bullet</div>
		<div style="position:absolute;top:440px; left:50px;width: 720px;font-size:0.8emM">Abstract API layer between Blender and its Physics Engine offers
            possibility of adding different physics engines (ODE, DVC3D, ...).</div>
        <reference>Video Courtesy of Binh Nguyen</reference>
    <details>
        <ul>
            <li>Es gibt auch Projekte, bei denen Bullet in Blender durch andere Physicengines ersetzt wurde, wie in dem folgenden Video</div>
            <li>Obwohl wir mit Bullet Physics benutzen, sollte man erwähnen dass MORSE nie dazu gedacht war, physicalisch akkurat zu sein, allerdings macht
                Blender es einem leicht, die Physikengine zu wechseln indem es eine abstrakte API Layer zwischen Blender und er Physikengine bietet.</li>
            <li>Wir selbst haben das allerdings nie versucht, aber es gibt PRojekte wie z.B. das DVC3D von Binh Nguyen, der eine Erweiterung von DVC2D (Phyisc speziell
                für Robotik/focuses on accurate robotics application)in Blender integriert hat</li>
        </ul>
    </details>
</section>


<section>
	<h2>MORSE specific features</h2>
	<ul>
		<li>Adjustable degree of realism</li>
		<li>Human avatar for HRI scenarios</li>
		<li>Completely scriptable via Python</li>
		<li>Blender for modeling</li>
		<li>Bullet physics engine</li>
		<li>Middleware independence</li>
		<li>Distributed multi-node simulation</li>
	</ul>
	<details>
		<ul>
			<li>Im Vergleich zu anderen Simulatoren hat MORSE ein paar spezielle Features.</li>
            <li>Eine der Haupt-Design-Entscheidungen für MORSE war es, die Detaillevel der Simulation steuern zu können. Jmd, der z.B. in der Computer Vision arbeitet,
                wird vielleicht weniger genaue Motion Controller brauchen, oder Leute, die in high level Planning arbeiten wollen sich nicht die Objekterkennung aus Sensordaten
                beschäftigen, sondern eher Perzeption des Roboters auf einer abstrakteren Ebene betrachten</li>
            <li>MORSE hat ausserdem einen simulierten Menschen, zu dem ich gleich noch mehr erzählen werden...</li>
            <li>... und kann in Python geskriptet werden</li>
            <li>Außerdem ist MORSE unabhängig von Middlewares und unterstützt auch auf mehreren Rechnern verteilte Simulationen mit mehreren Robotern</li>
		</ul>
	</details>
</section>

<section>
	<h2>MORSE specific features</h2>
	<ul>
		<li><strong>Adjustable degree of realism</strong></li>
		<li><strong>Human avatar for HRI scenarios</strong></li>
		<li>Completely scriptable via Python</li>
		<li>Blender for modeling</li>
		<li>Bullet physics engine</li>
		<li>Middleware independence</li>
		<li>Distributed multi-node simulation</li>
	</ul>
	<details>
		<ul>
			<li>Die zwei wichtigsten Gründe, warum wir uns für MORSE entschieden haben, waren der steuerbare Level des Realismus, da wir uns in unserer Grupper zu einem großem Teil
                mit high-level Planning beschäftigt haben ...</li>
            <li>... und die Tatsache, dass schon ein MOdell eines Menschen verfügbar war, das man während der Simulation des Roboters in Echtzeit steuern konnte.
		</ul>
	</details>
</section>

<section>
<h2>
<pre>
$ vim pr2_kitchen/defaults.py
</pre>
</h2>
<br/>
<div>


<pre class="prettyprint">
from morse.builder import *

robot = BasePR2() # Bare PR2 model
robot.add_default_interface('ros') # control PR2 via ROS

odometry = Odometry() # Sensor for Odometry information
odometry.add_interface('ros', topic="/odom")
robot.append(odometry)

keyboard = Keyboard() # Keyboard control
robot.append(keyboard)

scan = Hokuyo() # Hokuyo 2D laser scanner
scan.translate(x=0.275, z=0.252)
scan.add_interface('ros', topic='/scan')
robot.append(scan)

env = Environment("kitchen.blend") # load custom 3d model
</pre>
</div>
	<details>
		<ul>
			<li>Auch Simulationsszenarien werden bei MORSE in Python definiert, wofür es eine eigene API, die sogenannte "BUilder API" gibt.
                Wie das dann ungefähr aussieht, möchte ich an diesem kleinen Beispiel kurz zeigen. Hiel wollen wir mit einem Roboter eine 2D Gridmap
                einer Umgebung erstellen. Dazu (...) ERKLÄREN</li>
            <li>Und das ist schon alles an Code, was man braucht um loszulegen. Wenn wir das Szenario nun starten, sieht es folgendermaßen aus und wir
                könenn ...
		</ul>
	</details>
</section>

<section>
<h2>
<pre>
$ morse run pr2_kitchen
</pre>
</h2>
<div>
	<video muted style="position:absolute;top:150px; left:0px; width: 100%;" src="../media/videos/pr2_mapping.webm"></video> 
<!--img style="width:100%" src="../media/images/pr2_mapping.png"-->
</div>
<reference><em>Using standard ROS <tt>robot_state_publisher</tt> and <tt>gmapping</tt></em></reference>
</section>

<section>
	<h3>Available Robot Components</h3>
    <ul>
		<li>14 robots
		<li>24 sensors
		<li>21 actuators
		<li>2 human components
	</ul>
	<img style="position:absolute;top:320px;left: 120px; width: 70%;" src="../media/images/all_components_3rows.png"/>
	
<details>
    <ul>
        <li>Mit dieser Python API kann man beliebig alle bestehenden Komponenten kombinieren und sich seine Simulationsumgebungen zusammenbauen</li>
        <li>MORSE beinhaltet mittlerweile ...</li>
    </ul>
</details>
</section>


<section>
    <h3>MORSE Principles: Interfaces</h3>
    <ul style="font-size:0.8em">
         <li><b>RPC-oriented</b> or <b>datastream-oriented</b> communication</li>
         <li><b>Modifiers</b> alter the sensors/actuators data, typically to add noise</li>
         <li><b>Strong decoupling</b>: components are not aware of surrounding interfaces/modifiers</li>
         <li>Generic interfaces: <em>sockets</em>, <em>text</em>,
         <li>Supported <b>robotic middlewares</b>: <em>ROS, YARP, GenoM/Pocolibs, OpenRTM, MOOS</em>
         <li>Language <em>bindings</em> for Python. Easy to develop for other languages through the socket interface.
            <!--ul>
            <li><img style="height: 21px; margin-bottom: -5px;" src="../media/images/logo-ros.png" />
            <li>YARP
            <li>GenoM/Pocolibs
            <li>OpenRTM
            <li>MOOS
            </ul-->
    </ul>
<details>
<ul>
    <li>Zur Kommunikation bietet MORSE verschiedene Interfaces, einmal ein Data Stream basiertes und ein Remote Procedure Calls (synchron und asynchron)</li>
    <li>Es können sog. Modifiers verwendet werden um z.B. Rauschen auf Sensordaten zu legen.</li>
    <li>Starke Trennung zwischen Komponenten und ihren Modifiers / Middlewares</li>
    <li>standardmässig gibt es generische Interfaces wie Sockets und text ...</li>
    <li>Aber auch support für verschiedene Robotik Middleware wie ...</li>
    <li>Hier findet man auch zu großem Teil Beiträge von mir, da ich zusammen mit einem Kollegen vom LAAS CNRS aufgesetzt habe und mittlerweile von einigen
        Leuten (auch Firmen) verwendet wird.</li>
    <li>Als Programmiersprache für die Middlewares wird Python verwendet, allerdings ist es einfach, auch mit anderen Sprachen zu kommunizieren, z.B. durch
        das Socket Interface (bzw. C-Libraries in Python)
    
    <li>ROS: well, <strong>CONTRIBUTION:</strong>ROS middleware, mention problem Py2/3 etc.
    <li>GenoM/pocolibs: Generator of Modules and its communication layer, developed at LAAS
    <li>OpenRTM: Open source Robotic Technology Middleware, developed by Japan's National Institute of Advanced Industrial Science and Technology
    <li>MOOS: Mission Oriented Software Suite, developed at Oxford University
    <li>MORSE is middleware independant ! (bindings are modules)
</ul>
</details>
</section>

<section> 
	<h3>Simulated HRI Interface</h3>
	<ul style="font-size:0.8em">
		<li><strong>Intuitive</strong> Interface
		<li>Controllable via <strong>Keyboard and Mouse</strong> (or Kinect)
		<li>Enable <strong>pick- and place</strong> actions, support switches
		<li><strong>Realistic</strong> postures and actions
		<li>Simulated full body <strong>motion tracking</strong>
	</ul>
	<img style="position:absolute;top:380px; left:150px; width: 30%;" src="../media/images/morse_hri_3rd_person.png"/>
	<img style="position:absolute;top:380px; left:400px; width: 30%;" src="../media/images/morse_hri_1st_person.png"/>
	<details>
    <ul>
        <li>Eine weitere Conribution von mir in Zusammenarbeit mit einem Studenten ist das Interface für den simulierten Menschen.</li>
        <li>Als wir anfienge, den Simulator zu benutzen, war zwar schon ein steuerbares Modell eines Menschen vorhanden, allerdings war es in der
            Form nur äußerst schwer, pick- and place Aktionen auszuführen.</li>
        <li>Deswegen haben wir uns dazu entschlossen, ein Intuitive Steuerungsinterface für dein Menschen zu entwickeln, durch das wir den Menschen
            per Keyboard und Maus verwenden können um Objekte in der simulierten Umgebung bewegen zu können, Türen und Schubladen benutzen zu können und
            auch die Verwendung von Schaltern zu ermöglichen. </li>
        <li>Wir haben dabei auch darauf geachtet, dass die Körperhaltung des Menschen dabei möglichst realistisch ist und ein simuliertes full body
            motion tracking system integriert.
        <li>Dazu haben wir uns einige Ansätze von 3D Computerspielen abgeschaut und der Mensch ist nur so steuerbar, dass ihn von einer Kamera sieht, die
            kurz hinter ihm ist während man läuft (wie Bild links). Die Cam versucht auch zu vermeiden, dass die Sicht auf den Menschen durch Objekte verdeckt wird. Wenn
            man mit Objekten in der Umgebung interagieren möchte, wird in eine 1st Person Perspektive gewechselt und Aktionen eingeblendet, die für das
            jeweilige Objekt verfügar sind.
            
        <!--li>Control similar to 3D Computer games (Mouse for direction, keyboard for moving)
		<li>3rd person view behind avtar for movement, first person view for object interactions
		<li>interactive environment, coded with Blender BGE logic bricks
		<li>Objects placement using Blender Internal IK (ITasc)
		<li>Simulated object recognition using MORSE "semantic camera" and ROS TF</li-->
    </ul>
    </details>
</section>

<section>
    <figure>
        <video style="left:0px" src="../media/videos/hri.webm"></video> 
        <!--figcaption style="color:white">Human-Robot interaction</figcaption-->
    </figure>
    <details>
        <ul>
            <li>In AKtion sieht das ganze dann so aus...</li>
            <li>Wie man kurz gesehen hat, kann man sich auch alle Objekte anzeigen lassen, mit denen man interagieren kann...</li>
            <li>Der Mensch kann auch Objekte verdecken wie man hier sieht, das ist allerdings eher eine Eigenschaft des Sensors des Roboters</li>
        </ul>
    <details>
</section>

<section>
    <h3>Distributed Multi-Node Multi-Robot Simulation</h3>
    <video muted style="left:0px;width:800px" src="../media/videos/morse_multi.webm#t=15"></video>
<details>
<ul>
    <li>Es ist auch möglich, die Simulation auf mehrere Rechner zu verteilen, wie man in dem Video zum Beispiel sehen kann. Hier sieht man 2
        verschiedene Instanzen von MORSE, die über den "Multinode Server kommunizieren."</li>
    <li>Getestet haben wir das mit 10 nodes, hat geklappt</li>
    
    <!--li>Two MORSE instance running and a "multinode-server"
    <li>Each robot is controlled by a node
    <li>Tested with 10 nodes (needs to work on sync : WIP)</li-->
</ul>
</details>
</section>

<section>
    <h2>Still missing...</h2>

    <ul>
        <li>Faster-than-realtime simulation</li>
        <li>Explicit <em>timeline of events</em></li>
        <li>Formal support for logging/replay</li>
        <!--li>Full support of PR2</li-->
		<li>Force/torque sensores
        <li>...(you name it!)</li>
    </ul>
    <details>
        <ul>
            <li>Es gibt allerdinge auch einige Dinge, die ein MORSE noch nicht verfügar sind, allerdings gerne gewünsch wären...</li>
            <li>Mit dabei zum Beispiel ... </li>
        </ul>
    </details>
</section>

<section>
    <h2>Presentation outline</h2>
    <ul>
      <li style="color:grey;">The MORSE simulator
		<!--ul>
      		<li>Simulating with MORSE
      		<li>MORSE Principles
      		<li>Human-Robot Interaction
      		<li>Distributed simulations
      		<li>Still missing...
		</ul-->
	  <li>Low Cost Activity Recognition
		<ul>
			<li>Goals
			<li>Models of Human Activities
			<li>HHMM-based Activity Recognition
			<li>Video: Real World Application 
		</ul>
	  <li style="color:grey;">Student Project: HRI Interface for B21
    </ul>
     <details>
		<ul>
        <li>So weit zum MORSE Simulator, jetzt möchte ich noch etwas darüber erzählen, was ich während meiner Promotion außer Simulation noch so gemacht habe und auf
            einen Ansatz zur Erkennung von menschlichen Aktivitäten in Alltagssituationen eingehen.</li>
		</ul>
    </details>
</section>

<section>
	<h3>Low Cost Activity Recognition</h3>
		<!--ul>
			<li>Scenario: Robots working together with humans in domestic environments
			<li>Goal: Include models of human activities into robot planning
			<li>Human Motion tracking using Kinect sensors 
		</ul-->	
		<div style="text-align: center;">
		<img style="position:absolute;top:170px; left:100px; width: 85%;" src="../media/images/intro_1.png"/>
		</div>	
		 <details>
			<ul>
	        <li>Das Bild hier zeigt ganz gut, was wir uns als Ziel gesetzt haben...</li>
            <li>Wir sehen hier eine Roboter in einer typischen Küchen und Ich bin gerade dabei den Tisch zu decken. Die Aufgabe des Roboter ist es hierbei erstmal nur
                basierend auf Motion Tracking Daten seiner Kinect zu erkennen, was ich gerade mache.<li>
			</ul>
	    </details>
</section>

<section>
<h3>Goals</h3>
<ul style="font-size:0.8em">
	<li>Robot should be <b>aware of human task execution</b> to enable more human aware and adaptive planning</li>
	<li><b>Inexpensive and unobtrusive</b> sensor setup</li>
    <li>Activity models obtained from observing <b>habitual behavior</b></li>
	<li><b>Simple and noise-resistant</b> features</li>
	<!--li>Anticipate likely future actions/activities</li-->
</ul>
<details>
    <ul>
        <li>Wir wollen das unser Roboter besser auf seinen menschlichen Partner eingeht und ihn bei der Planung seiner Aktionen berücksichtigt und dazu ...</li>
        <li>... denke wir, dass er zunächst eine Ahnung davon haben sollte, was der Mensch tut.</li>
        <li>Eine weitere Prämisse war, dass unser Sensor Setup billig und wenig aufdringlich sein sollte. Im AR Bereich: Viel RFID auf OBJ, komplexe Motion Tracking
            Systeme, das wollten wir gerade nicht</li>
        <li>Außerdem wollten wir, dass der Roboter Modell von meschlichen Gewohnheiten lernt um eine Ahnung zu bekommen, was der Mensch normalerweise an bestimmten Orten tut.</li>
        <li>... und es sollten einfache Features verwendet werden, die unanfällug gegen Rauschen sind, welches wir bei billigeren Sensoren erwarten.</li>
        <li>...</li>
    </ul>
</details>
</section>

<section>
	<h3>Models of Human Activities</h3>
	<ul style="font-size:0.8em">
		<li><strong>Spatial model</strong> representing regions where humans generally are located during pick- and place tasks</li>
		<li>Learned from observing <strong>motion tracking data</strong> during human task execution</li>
		<!--li>Use High-Level Hierarchical HMM to perform Activity Recognition observing human task performance</li-->
			<img style="position:absolute;top:330px; left:100px; width: 30%;" src="../media/images/SPRAM_data_overview.png"/>
			<img style="position:absolute;top:365px; left:420px; width: 30%;" src="../media/images/full_spatial_model_1.png"/>
	</ul>
    <details>
        <li>Als Grundlage für unsere Daten haben wir (auf einem bereits vorhandenen datensatz) gelernt, wo Menschen normalerweise stehen wenn
            sie Objekte aus Schränken, Schubladen oder ähnlichen Lagerplätzen holen.</li>
        <li>
    </details>
</section>

<section>
	<h3>HHMM-based Activity Recognition</h3>
		<ul style="font-size:0.8em">
			<li>Generate high-Level <strong>hierarchical HMM (HHMM)</strong> to perform activity recognition observing human task performance</li>
			<li>Use <b>Forward-Backward Algorithm</b> to calculate posterior marginals of internal states of HHMM</li>
		</ul>
			<img style="position:absolute;top:320px; left:150px; width: 60%;" src="../media/images/hhmm.png"/>

</section>

<section>
	<h3>Scenario: Human Morning Routine</h3>
    <img style="position:absolute;top:120px; left:150px; width: 60%;" src="../media/images/SPRAM_dataset.png"/>
    
</section>

<section>
	<figure>
	    <video muted style="left:0px" src="../media/videos/LoCoAcRe_sml.webm"></video> 
	</figure>
</section>

<section>
    <h2>Presentation outline</h2>
    <ul>
      <li style="color:grey;">The MORSE simulator
		<!--ul>
      		<li>Simulating with MORSE
      		<li>MORSE Principles
      		<li>Human-Robot Interaction
      		<li>Distributed simulations
      		<li>Still missing...
		</ul-->
	  <li style="color:grey;">Low Cost Activity Recognition
		<!--ul>
			<li>Low Cost Activity Recognition
			<li>A General Framework for expectations
			<li>Simulated Application Scenarios
			<li>Real World Application 
		</ul-->
	  <li>Student Project: HRI Interface for B21
    </ul>
     <details>
		<ul>
        <li>As a last example, show student project...</li>
		</ul>
    </details>
</section>

<section>
	<h3>Goals</h3>
	<ul style="font-size:0.8em">
		<li>GUI for Robot control via natural language
		<li>Selection of Speech Recognition (pocketsphinx VS Google)
		<li>Grammatical analysis of commands	
		<li>Interface to knowledge base (KnowRob)
		<li>Include (Emergency) Stop of robot
	</ul>
	<img style="position:absolute;top:350px; left:200px; width: 50%;" src="../media/images/leon_gui.png"/>
</section>


<section>
<figure>
    <video style="left:0px" src="../media/videos/hri_interface_720p.webm"></video> 
</figure>
</section>


<section>
<h3>Other Projects</h3>
	<ul style="font-size:0.8em">
		<li>Learning models of human activities
		<li>Motion tracking dataset about human morning routine
		<li>Expectations framework for human-aware, reactive planning
		<li>Multi floor graph-based SLAM
	</ul>	
	<img style="position:absolute;top:420px; left:40px; height: 18%;" src="../media/images/clustering.png"/>
	<img style="position:absolute;top:420px; left:192px; height: 18%;" src="../media/images/dataset.png"/>
	<img style="position:absolute;top:420px; left:320px; height: 18%;" src="../media/images/jido_confused.png"/>
	<img style="position:absolute;top:420px; left:450px; height: 18%;" src="../media/images/multi_floor_slam_1.png"/>
	<img style="position:absolute;top:420px; left:620px; height: 18%;" src="../media/images/multi_floor_slam_2.png"/>

</section>

<section id='end'>
    <h3>Questions?</h3>
	<img align="center" style="position:absolute;top:30%; left:270px; height: 40%;" src="../media/images/qrcode_hcai.png"/>
	<h3 style="position:absolute;top: 400px;">http://hcai.in.tum.de/members/kargm</h3>
</section>

<!-- Your Style -->
<!-- Define the style of your presentation -->

<script>
//$("img#taxonomy").axzoomer({
//    'maxZoom':4
//    });
//$("img#taxonomy").smoothZoom({
//    width:800,
//    height:600,
//    border_SIZE:0});
</script>

<!-- Maybe a font from http://www.google.com/webfonts ? -->
<!--<link href='http://fonts.googleapis.com/css?family=Actor' rel='stylesheet'>-->

<link href='parser.css' rel='stylesheet'>

<style>
    @font-face {
    font-family: 'Lato';
    font-style: normal;
    font-weight: 400;
    src: url('Lato.ttf');
    }

  html, .view body { background-color: black; counter-reset: slideidx; }
  /*For the printout: html, .view body { background-color: white; counter-reset: slideidx; }*/
  body, .view section { background-color: white; border-radius: 12px }
  /*For the printout: body, .view section { background-color: white; border:2px solid black;border-radius: 12px;}*/
  /* A section is a slide. It's size is 800x600, and this will never change */
  section, .view head > title {
      /* The font from Google */
      font-family: 'Lato', arial, serif;
      font-size: 30px;
  }

  .view section:after {
    counter-increment: slideidx;
    content: counter(slideidx, decimal-leading-zero);
    position: absolute; bottom: -80px; right: 100px;
    color: white;
  }

  .view head > title {
    color: white;
    text-align: center;
    margin: 1em 0 1em 0;
  }

  h1, h2 {
    margin-top: 100px;
    text-align: left;
    font-size: 40px;
    margin-left: 40px;
  }
  h3 {
    margin: 50px 0 50px 100px;
  }

  ul {
      margin: 50px 150px;
  }
  ul ul {
      margin: 20px 20px;
      font-size: 0.8em;
  }


  p {
    margin: 75px;
    font-size: 50px;
}

  pre {
      font-size: 0.6em;
      margin-left: 10px;
      padding-left: 5px;
      /*border-left: 1px solid;*/
  }

  blockquote {
    height: 100%;
    background-color: black;
    color: white;
    font-size: 60px;
    padding: 50px;
  }
  blockquote:before {
    content: open-quote;
  }
  blockquote:after {
    content: close-quote;
  }

  .note {
      position:absolute;
      border-radius:5px;
      border:1px solid;
      background:rgba(255, 255, 255, 0.9);
      padding:3px;
      font-size:10px;
   }
  /* Figures are displayed full-page, with the caption
     on top of the image/video */
  figure {
    background-color: white;
    width: 100%;
    height: 100%;
  }
  figure > * {
    position: absolute;
  }
  figure > img, figure > video {
    top: 10%; 
    /* Set up proportionate scaling */
    width: 100%;
    height: auto;
  }
  figure.vertical > img, figure.vertical > video {
    left: 10%; 
    top:0%;
    /* Set up proportionate scaling */
    height: 100%;
    width: auto;
  }
  figcaption {
    margin: 70px;
    font-size: 30px;
  }
  fignote {
  bottom:0;
  right:0;
    margin: 70px;
    font-size: 16px;
  }

  reference {
  bottom:0;
  right:0;
  margin: 40px;
  font-size: 14px;
  position:absolute;
  }


  footer {
    position: absolute;
    bottom: 0;
    width: 100%;
    padding: 20px;
    text-align: right;
    background-color: #F3F4F8;
    border-top: 1px solid #CCC;
  }


table {
        overflow:hidden;
        border:1px solid #d3d3d3;
        background:#fefefe;
        width:70%;
        margin:5% auto 0;
        -moz-border-radius:5px; /* FF1+ */
        -webkit-border-radius:5px; /* Saf3-4 */
        border-radius:5px;
    }
    
    th, td {padding:3px 5px 3px; text-align:center; }
    
    th {padding-top:8px; background:#e8eaeb;}
    

  /* Transition effect */
  /* Feel free to change the transition effect for original
     animations. See here:
     https://developer.mozilla.org/en/CSS/CSS_transitions
     How to use CSS3 Transitions: */
  section {
    -moz-transition: left 0ms linear 0s;
    -webkit-transition: left 400ms linear 0s;
    -ms-transition: left 400ms linear 0s;
    transition: left 400ms linear 0s;
  }
  .view section {
    -moz-transition: none;
    -webkit-transition: none;
    -ms-transition: none;
    transition: none;
  }

  .view section[aria-selected] {
    border: 5px red solid;
  }

  /* Before */
  section { left: -150%; }
  /* Now */
  section[aria-selected] { left: 0; }
  /* After */
  section[aria-selected] ~ section { left: +150%; }

  /* Incremental elements */

  /* By default, visible */
  .incremental > * { opacity: 0.5; }

  /* The current item */
  .incremental > *[aria-selected] { opacity: 1; }

  /* The items to-be-selected */
  .incremental > *[aria-selected] ~ * { opacity: 0; }

  /* The progressbar, at the bottom of the slides, show the global
     progress of the presentation. */
  #progress-bar {
    font-family: 'Actor', arial, serif;
    height: 25px;
    /*background: #AAA;*/
  }
</style>

<!-- {{{{ dzslides core
#
#
#     __  __  __       .  __   ___  __
#    |  \  / /__` |    | |  \ |__  /__`
#    |__/ /_ .__/ |___ | |__/ |___ .__/ core :€
#
#
# The following block of code is not supposed to be edited.
# But if you want to change the behavior of these slides,
# feel free to hack it!
#
-->

<div id="progress-bar"></div>

<!-- Default Style -->
<style>
  * { margin: 0; padding: 0; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; box-sizing: border-box; }
  details { display: none; }
  body {
    width: 800px; height: 600px;
    margin-left: -400px; margin-top: -300px;
    position: absolute; top: 50%; left: 50%;
    overflow: hidden;
    display: none;
  }
  .view body {
    position: static;
    margin: 0; padding: 0;
    width: 100%; height: 100%;
    display: inline-block;
    overflow: visible; overflow-x: hidden;
    /* undo Dz.onresize */
    transform: none !important;
    -moz-transform: none !important;
    -webkit-transform: none !important;
    -o-transform: none !important;
    -ms-transform: none !important;
  }
  .view head, .view head > title { display: block }
  section {
    position: absolute;
    pointer-events: none;
    width: 100%; height: 100%;
  }
  .view section {
    pointer-events: auto;
    position: static;
    width: 800px; height: 600px;
    margin: -150px -200px;
    float: left;

    transform: scale(.4);
    -moz-transform: scale(.4);
    -webkit-transform: scale(.4);
    -o-transform: scale(.4);
    -ms-transform: scale(.4);
  }
  .view section > * { pointer-events: none; }
  section[aria-selected] { pointer-events: auto; }
  html { overflow: hidden; }
  html.view { overflow: visible; }
  body.loaded { display: block; }
  .incremental {visibility: hidden; }
  .incremental[active] {visibility: visible; }
  #progress-bar{
    bottom: 0;
    position: absolute;
    width:50px;
    right: 0;
    -moz-transition: width 400ms linear 0s;
    -webkit-transition: width 400ms linear 0s;
    -ms-transition: width 400ms linear 0s;
    transition: width 400ms linear 0s;
  }
  .view #progress-bar {
    display: none;
  }
</style>

<script>
  var Dz = {
    remoteWindows: [],
    idx: -1,
    step: 0,
    html: null,
    slides: null,
    progressBar : null,
    params: {
      autoplay: "1"
    }
  };

  Dz.init = function() {
    document.body.className = "loaded";
    this.slides = Array.prototype.slice.call($$$$("body > section"));
    this.endslide = $$$("#end");
    this.endidx = this.slides.indexOf(this.endslide);
    this.progressBar = $$$("#progress-bar");
    this.html = document.body.parentNode;
    this.setupParams();
    this.onhashchange();
    this.setupTouchEvents();
    this.onresize();
    this.setupView();
  }

  Dz.setupParams = function() {
    var p = window.location.search.substr(1).split('&');
    p.forEach(function(e, i, a) {
      var keyVal = e.split('=');
      Dz.params[keyVal[0]] = decodeURIComponent(keyVal[1]);
    });
  // Specific params handling
    if (!+this.params.autoplay)
      $$$$.forEach($$$$("video"), function(v){ v.controls = true });
  }

  Dz.onkeydown = function(aEvent) {
    // Don't intercept keyboard shortcuts
    if (aEvent.altKey
      || aEvent.ctrlKey
      || aEvent.metaKey
      || aEvent.shiftKey) {
      return;
    }
    if ( aEvent.keyCode == 37 // left arrow
      || aEvent.keyCode == 38 // up arrow
      || aEvent.keyCode == 33 // page up
    ) {
      aEvent.preventDefault();
      this.back();
    }
    if ( aEvent.keyCode == 39 // right arrow
      || aEvent.keyCode == 40 // down arrow
      || aEvent.keyCode == 34 // page down
    ) {
      aEvent.preventDefault();
      this.forward();
    }
    if (aEvent.keyCode == 35) { // end
      aEvent.preventDefault();
      this.goEnd();
    }
    if (aEvent.keyCode == 36) { // home
      aEvent.preventDefault();
      this.goStart();
    }
    if (aEvent.keyCode == 32) { // space
      aEvent.preventDefault();
      this.toggleContent();
    }
    if (aEvent.keyCode == 70) { // f
      aEvent.preventDefault();
      this.goFullscreen();
    }
    if (aEvent.keyCode == 79) { // o
      aEvent.preventDefault();
      this.toggleView();
    }
  }

  /* Touch Events */

  Dz.setupTouchEvents = function() {
    var orgX, newX;
    var tracking = false;

    var db = document.body;
    db.addEventListener("touchstart", start.bind(this), false);
    db.addEventListener("touchmove", move.bind(this), false);

    function start(aEvent) {
      aEvent.preventDefault();
      tracking = true;
      orgX = aEvent.changedTouches[0].pageX;
    }

    function move(aEvent) {
      if (!tracking) return;
      newX = aEvent.changedTouches[0].pageX;
      if (orgX - newX > 100) {
        tracking = false;
        this.forward();
      } else {
        if (orgX - newX < -100) {
          tracking = false;
          this.back();
        }
      }
    }
  }

  Dz.setupView = function() {
    document.body.addEventListener("click", function ( e ) {
      if (!Dz.html.classList.contains("view")) return;
      if (!e.target || e.target.nodeName != "SECTION") return;

      Dz.html.classList.remove("view");
      Dz.setCursor(Dz.slides.indexOf(e.target) + 1);
    }, false);
  }

  /* Adapt the size of the slides to the window */

  Dz.onresize = function() {
    var db = document.body;
    var sx = db.clientWidth / window.innerWidth;
    var sy = db.clientHeight / window.innerHeight;
    var transform = "scale(" + (1/Math.max(sx, sy)) + ")";

    db.style.MozTransform = transform;
    db.style.WebkitTransform = transform;
    db.style.OTransform = transform;
    db.style.msTransform = transform;
    db.style.transform = transform;
  }


  Dz.getDetails = function(aIdx) {
    var s = $$$("section:nth-of-type(" + aIdx + ")");
    var d = s.$$$("details");
    return d ? d.innerHTML : "";
  }

  Dz.onmessage = function(aEvent) {
    var argv = aEvent.data.split(" "), argc = argv.length;
    argv.forEach(function(e, i, a) { a[i] = decodeURIComponent(e) });
    var win = aEvent.source;
    if (argv[0] === "REGISTER" && argc === 1) {
      this.remoteWindows.push(win);
      this.postMsg(win, "REGISTERED", document.title, this.slides.length);
      this.postMsg(win, "CURSOR", this.idx + "." + this.step);
      return;
    }
    if (argv[0] === "BACK" && argc === 1)
      this.back();
    if (argv[0] === "FORWARD" && argc === 1)
      this.forward();
    if (argv[0] === "START" && argc === 1)
      this.goStart();
    if (argv[0] === "END" && argc === 1)
      this.goEnd();
    if (argv[0] === "TOGGLE_CONTENT" && argc === 1)
      this.toggleContent();
    if (argv[0] === "SET_CURSOR" && argc === 2)
      window.location.hash = "#" + argv[1];
    if (argv[0] === "GET_CURSOR" && argc === 1)
      this.postMsg(win, "CURSOR", this.idx + "." + this.step);
    if (argv[0] === "GET_NOTES" && argc === 1)
      this.postMsg(win, "NOTES", this.getDetails(this.idx));
  }

  Dz.toggleContent = function() {
    // If a Video is present in this new slide, play it.
    // If a Video is present in the previous slide, stop it.
    var s = $$$("section[aria-selected]");
    if (s) {
      var video = s.$$$("video");
      if (video) {
        if (video.ended || video.paused) {
          video.play();
        } else {
          video.pause();
        }
      }
    }
  }

  Dz.setCursor = function(aIdx, aStep) {
    // If the user change the slide number in the URL bar, jump
    // to this slide.
    aStep = (aStep != 0 && typeof aStep !== "undefined") ? "." + aStep : ".0";
    window.location.hash = "#" + aIdx + aStep;
  }

  Dz.onhashchange = function() {
    var cursor = window.location.hash.split("#"),
        newidx = 1,
        newstep = 0;
    if (cursor.length == 2) {
      newidx = ~~cursor[1].split(".")[0];
      newstep = ~~cursor[1].split(".")[1];
      if (newstep > Dz.slides[newidx - 1].$$$$('.incremental > *').length) {
        newstep = 0;
        newidx++;
      }
    }
    this.setProgress(newidx, newstep);
    if (newidx != this.idx) {
      this.setSlide(newidx);
    }
    if (newstep != this.step) {
      this.setIncremental(newstep);
    }
    for (var i = 0; i < this.remoteWindows.length; i++) {
      this.postMsg(this.remoteWindows[i], "CURSOR", this.idx + "." + this.step);
    }
  }

  Dz.back = function() {
    if (this.idx == 1 && this.step == 0) {
      return;
    }
    if (this.step == 0) {
      this.setCursor(this.idx - 1,
                     this.slides[this.idx - 2].$$$$('.incremental > *').length);
    } else {
      this.setCursor(this.idx, this.step - 1);
    }
  }

  Dz.forward = function() {
    if (this.idx >= this.slides.length &&
        this.step >= this.slides[this.idx - 1].$$$$('.incremental > *').length) {
        return;
    }
    if (this.step >= this.slides[this.idx - 1].$$$$('.incremental > *').length) {
      this.setCursor(this.idx + 1, 0);
    } else {
      this.setCursor(this.idx, this.step + 1);
    }
  }

  Dz.goStart = function() {
    this.setCursor(1, 0);
  }

  Dz.goEnd = function() {
    var lastIdx = this.slides.length;
    var lastStep = this.slides[lastIdx - 1].$$$$('.incremental > *').length;
    this.setCursor(lastIdx, lastStep);
  }

  Dz.toggleView = function() {
    this.html.classList.toggle("view");

    if (this.html.classList.contains("view")) {
      $$$("section[aria-selected]").scrollIntoView(true);
    }
  }

  Dz.setSlide = function(aIdx) {
    this.idx = aIdx;
    var old = $$$("section[aria-selected]");
    var next = $$$("section:nth-of-type("+ this.idx +")");
    if (old) {
      old.removeAttribute("aria-selected");
      var videos = old.$$$$("video");
      for (var i = 0 ; i < videos.length; i++) {
        videos[i].pause();
      }
    }
    if (next) {
      next.setAttribute("aria-selected", "true");
      if (this.html.classList.contains("view")) {
        next.scrollIntoView();
      }
      var videos = next.$$$$("video");
      for (var i = 0 ; i < videos.length; i++) {
      
        if (videos[i] && !!+this.params.autoplay) {
         videos[i].play();
        }
      }
    } else {
      // That should not happen
      this.idx = -1;
      // console.warn("Slide doesn't exist.");
    }
  }

  Dz.setIncremental = function(aStep) {
    this.step = aStep;
    var old = this.slides[this.idx - 1].$$$('.incremental > *[aria-selected]');
    if (old) {
      old.removeAttribute('aria-selected');
    }
    var incrementals = $$$$('.incremental');
    if (this.step <= 0) {
      $$$$.forEach(incrementals, function(aNode) {
        aNode.removeAttribute('active');
      });
      return;
    }
    var next = this.slides[this.idx - 1].$$$$('.incremental > *')[this.step - 1];
    if (next) {
      next.setAttribute('aria-selected', true);
      next.parentNode.setAttribute('active', true);
      var found = false;
      $$$$.forEach(incrementals, function(aNode) {
        if (aNode != next.parentNode)
          if (found)
            aNode.removeAttribute('active');
          else
            aNode.setAttribute('active', true);
        else
          found = true;
      });
    } else {
      setCursor(this.idx, 0);
    }
    return next;
  }

  Dz.goFullscreen = function() {
    var html = $$$('html'),
        requestFullscreen = html.requestFullscreen || html.requestFullScreen || html.mozRequestFullScreen || html.webkitRequestFullScreen;
    if (requestFullscreen) {
      requestFullscreen.apply(html);
    }
  }
  
  Dz.setProgress = function(aIdx, aStep) {
    var slide = $$$("section:nth-of-type("+ aIdx +")");
    if (!slide)
      return;
    var steps = slide.$$$$('.incremental > *').length + 1,
        slideSize = 100 / (this.slides.length - 1),
        stepSize = slideSize / steps;
        //this.progressBar.style.width = ((aIdx - 1) * slideSize + aStep * stepSize) + '%';
    if (aIdx == 1) // no numbering on first slide
        this.progressBar.innerHTML = "";
    else
        this.progressBar.innerHTML = (aIdx - 1) + "/" + this.endidx;
  }
  
  Dz.postMsg = function(aWin, aMsg) { // [arg0, [arg1...]]
    aMsg = [aMsg];
    for (var i = 2; i < arguments.length; i++)
      aMsg.push(encodeURIComponent(arguments[i]));
    aWin.postMessage(aMsg.join(" "), "*");
  }

  function init() {
    Dz.init();
    window.onkeydown = Dz.onkeydown.bind(Dz);
    window.onresize = Dz.onresize.bind(Dz);
    window.onhashchange = Dz.onhashchange.bind(Dz);
    window.onmessage = Dz.onmessage.bind(Dz);

    prettyPrint()
  }

  window.onload = init;
</script>


<script> // Helpers
  if (!Function.prototype.bind) {
    Function.prototype.bind = function (oThis) {

      // closest thing possible to the ECMAScript 5 internal IsCallable
      // function 
      if (typeof this !== "function")
      throw new TypeError(
        "Function.prototype.bind - what is trying to be fBound is not callable"
      );

      var aArgs = Array.prototype.slice.call(arguments, 1),
          fToBind = this,
          fNOP = function () {},
          fBound = function () {
            return fToBind.apply( this instanceof fNOP ? this : oThis || window,
                   aArgs.concat(Array.prototype.slice.call(arguments)));
          };

      fNOP.prototype = this.prototype;
      fBound.prototype = new fNOP();

      return fBound;
    };
  }

  var $$$ = (HTMLElement.prototype.$$$ = function(aQuery) {
    return this.querySelector(aQuery);
  }).bind(document);

  var $$$$ = (HTMLElement.prototype.$$$$ = function(aQuery) {
    return this.querySelectorAll(aQuery);
  }).bind(document);

  $$$$.forEach = function(nodeList, fun) {
    Array.prototype.forEach.call(nodeList, fun);
  }

</script>
<!-- vim: set fdm=marker: }}} -->

